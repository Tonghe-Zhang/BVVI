Start BVVI test. Current time=2024-03-29-00-19-12
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
test Beta Vector Value Iteration.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
hyper parameters:{}
sizes:
  size_of_action_space: 2
  size_of_state_space: 3
  size_of_observation_space: 3
  horizon_len: 3
  num_episode: 5
  confidence_level: 0.2
  discount_factor: 1.0

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Call function '  beta_vector_value_iteration...' 
				POLICY NORMALIZATION TEST:True


	Into episode 0/5=0.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 2/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 1/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 0/3
		Enter parameter learning
				POLICY NORMALIZATION TEST:True
	End of episode 0. policy's tested_returns[0]=6.987546202615401, mu_err[0]=0.33743955231058037, T_err[0]=0.05376583294180999, O_err[0]=0.06887034355842381
	Successfuly saved newest kernels and policies to folder: ./learnt


	Into episode 1/5=20.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 2/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 1/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 0/3
		Enter parameter learning
				POLICY NORMALIZATION TEST:True
	End of episode 1. policy's tested_returns[1]=7.4004158409670655, mu_err[1]=0.10874448382902689, T_err[1]=0.0527855395022948, O_err[1]=0.06942054985022326
	Successfuly saved newest kernels and policies to folder: ./learnt


	Into episode 2/5=40.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 2/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 1/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 0/3
		Enter parameter learning
				POLICY NORMALIZATION TEST:True
	End of episode 2. policy's tested_returns[2]=6.405445802663789, mu_err[2]=0.05003357038853204, T_err[2]=0.051890648751846104, O_err[2]=0.06840332012113226
	Successfuly saved newest kernels and policies to folder: ./learnt


	Into episode 3/5=60.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 2/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 1/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 0/3
		Enter parameter learning
				POLICY NORMALIZATION TEST:True
	End of episode 3. policy's tested_returns[3]=7.964183562889727, mu_err[3]=0.04971906153324351, T_err[3]=0.04703960517164609, O_err[3]=0.0674275573890973
	Successfuly saved newest kernels and policies to folder: ./learnt


	Into episode 4/5=80.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 2/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 1/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 0/3
		Enter parameter learning
				POLICY NORMALIZATION TEST:True
	End of episode 4. policy's tested_returns[4]=8.85424082769794, mu_err[4]=0.06272673467309532, T_err[4]=0.046292063589863805, O_err[4]=0.06049287549271646
	Successfuly saved newest kernels and policies to folder: ./learnt
End of training. number of iters K=5
'  beta_vector_value_iteration...' returned.
End BVVI test. Current time=2024-03-29-00-19-17
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Call function '  visualize_performance...' 
'  visualize_performance...' returned.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Beta Vector Value Iteration test complete.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
[1;31m---------------------------------------------------------------------------[0m
[1;31mValueError[0m                                Traceback (most recent call last)
File [1;32mD:\Research\Huang\Rebuttal\Experiment\BVVI\main.py:375[0m, in [0;36m<module>[1;34m[0m
[0;32m    372[0m model_true_load[38;5;241m=[39m(mu_true, T_true, O_true)
[0;32m    373[0m reward_true_load[38;5;241m=[39mR_true
[1;32m--> 375[0m policy_learnt[38;5;241m=[39m[43mmain[49m[43m([49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m[43mmodel_true_load[49m[43m,[49m[43mreward_true_load[49m[43m,[49m[38;5;28;43;01mNone[39;49;00m[43m,[49m[38;5;28;43;01mNone[39;49;00m[43m,[49m[38;5;124;43m'[39;49m[38;5;124;43mlearnt[39;49m[38;5;130;43;01m\\[39;49;00m[38;5;124;43mnaive[39;49m[38;5;124;43m'[39;49m[43m)[49m
[0;32m    377[0m [38;5;28;01mfrom[39;00m [38;5;21;01mfunc[39;00m [38;5;28;01mimport[39;00m short_test
[0;32m    378[0m short_test(policy_learnt,mu_true,T_true,O_true,R_true,only_reward[38;5;241m=[39m[38;5;28;01mFalse[39;00m)

File [1;32mD:\Research\Huang\Rebuttal\Experiment\BVVI\main.py:348[0m, in [0;36mmain[1;34m(output_to_log_file, train_from_scratch, model_true_load, reward_true_load, model_load, policy_load, weight_output_parent_directory, config_filename, log_episode_filename)[0m
[0;32m    346[0m     [38;5;66;03m# log_episode_file.write(f"\n\nEnd Testing BVVI. Current time={current_time_str()}")[39;00m
[0;32m    347[0m     log_episode_file[38;5;241m.[39mclose()
[1;32m--> 348[0m episode_data[38;5;241m=[39m[43mnp[49m[38;5;241;43m.[39;49m[43mloadtxt[49m[43m([49m[38;5;124;43m'[39;49m[38;5;124;43mlog[39;49m[38;5;130;43;01m\\[39;49;00m[38;5;124;43m'[39;49m[38;5;241;43m+[39;49m[43mlog_episode_filename[49m[38;5;241;43m+[39;49m[38;5;124;43m'[39;49m[38;5;124;43m.txt[39;49m[38;5;124;43m'[39;49m[43m,[49m[43m [49m[43mdtype[49m[38;5;241;43m=[39;49m[43mnp[49m[38;5;241;43m.[39;49m[43mfloat64[49m[43m)[49m
[0;32m    349[0m [38;5;28mprint[39m([38;5;124m'[39m[38;5;130;01m\'[39;00m[38;5;124m  beta_vector_value_iteration...[39m[38;5;130;01m\'[39;00m[38;5;124m returned.[39m[38;5;124m'[39m)
[0;32m    350[0m [38;5;28mprint[39m([38;5;124mf[39m[38;5;124m"[39m[38;5;124mEnd BVVI test. Current time=[39m[38;5;132;01m{[39;00mcurrent_time_str()[38;5;132;01m}[39;00m[38;5;124m"[39m)

File [1;32mD:\Softwares\Anaconda\lib\site-packages\numpy\lib\npyio.py:1159[0m, in [0;36mloadtxt[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)[0m
[0;32m   1157[0m     words [38;5;241m=[39m usecols_getter(words)
[0;32m   1158[0m [38;5;28;01melif[39;00m [38;5;28mlen[39m(words) [38;5;241m!=[39m ncols:
[1;32m-> 1159[0m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[0;32m   1160[0m         [38;5;124mf[39m[38;5;124m"[39m[38;5;124mWrong number of columns at line [39m[38;5;132;01m{[39;00mlineno[38;5;132;01m}[39;00m[38;5;124m"[39m)
[0;32m   1161[0m [38;5;66;03m# Convert each value according to its column, then pack it[39;00m
[0;32m   1162[0m [38;5;66;03m# according to the dtype's nesting, and store it.[39;00m
[0;32m   1163[0m chunk[38;5;241m.[39mappend(packer(convert_row(words)))

[1;31mValueError[0m: Wrong number of columns at line 6


Will output log information to both the file:console_output.log and the console.
Start BVVI test. Current time=2024-03-29-00-18-06
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
test Beta Vector Value Iteration.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
hyper parameters:{}
sizes:
  size_of_action_space: 2
  size_of_state_space: 3
  size_of_observation_space: 3
  horizon_len: 3
  num_episode: 5
  confidence_level: 0.2
  discount_factor: 1.0

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Call function '  beta_vector_value_iteration...' 
				POLICY NORMALIZATION TEST:True


	Into episode 0/5=0.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 2/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 1/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 0/3
		Enter parameter learning
				POLICY NORMALIZATION TEST:True
	End of episode 0. policy's tested_returns[0]=8.0648954294421, mu_err[0]=0.23621798416491765, T_err[0]=0.05481616807825978, O_err[0]=0.06597016327238442
	Successfuly saved newest kernels and policies to folder: ./learnt


	Into episode 1/5=20.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 2/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 1/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 0/3
		Enter parameter learning
				POLICY NORMALIZATION TEST:True
	End of episode 1. policy's tested_returns[1]=6.931036759823631, mu_err[1]=0.13726876270783597, T_err[1]=0.05853845753495513, O_err[1]=0.06202845790886492
	Successfuly saved newest kernels and policies to folder: ./learnt


	Into episode 2/5=40.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 2/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 1/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 0/3
		Enter parameter learning
				POLICY NORMALIZATION TEST:True
	End of episode 2. policy's tested_returns[2]=7.7486016288677275, mu_err[2]=0.0397231449678271, T_err[2]=0.055500735339555386, O_err[2]=0.06477226586409808
	Successfuly saved newest kernels and policies to folder: ./learnt


	Into episode 3/5=60.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 2/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 1/3
			 update Q function...
			 update value function...
			 update greedy policy...
				POLICY NORMALIZATION TEST:True
			 update beta vector...
			 Horizon remains: 0/3
		Enter parameter learning
				POLICY NORMALIZATION TEST:True
	End of episode 3. policy's tested_returns[3]=6.347910457273352, mu_err[3]=0.10268377510797842, T_err[3]=0.05309703799433611, O_err[3]=0.05849473884914823
	Successfuly saved newest kernels and policies to folder: ./learnt


	Into episode 4/5=80.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
