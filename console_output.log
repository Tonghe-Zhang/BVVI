%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
test Beta Vector Value Iteration.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
hyper parameters:{}
sizes:
  size_of_action_space: 2
  size_of_state_space: 3
  size_of_observation_space: 4
  horizon_len: 5
  num_episode: 5       
  confidence_level: 0.1
  discount_factor: 0.8

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Call function '  beta_vector_value_iteration...' 
Into episode 0/5=0.00%
		 belief propagation starts...
		 belief propagation ends...
		 dynamic programming starts...
			 update Q function...
			 update value function...
			 update greedy policy...
			 update beta vector...
		finish horizon 4/5
			 update Q function...
			 update value function...
			 update greedy policy...
			 update beta vector...
		finish horizon 3/5
			 update Q function...
			 update value function...
			 update greedy policy...
			 update beta vector...
		finish horizon 2/5
			 update Q function...
			 update value function...
			 update greedy policy...
			 update beta vector...
		finish horizon 1/5
			 update Q function...
			 update value function...
			 update greedy policy...
			 update beta vector...
		finish horizon 0/5
Enter parameter learning
h=0, action_distribution=tensor([0.3479, 0.0000], dtype=torch.float64), shape=torch.Size([2])
